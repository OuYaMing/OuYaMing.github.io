---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

---
# ğŸ‘¤ About me
---
I am **Yaming Ou** (**æ¬§äºšæ˜**), a **3rd-year Ph.D.** student with the Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences (**[CASIA](http://www.ia.cas.cn/)**), Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences (**[UCAS](https://www.ucas.edu.cn/)**), Beijing 100049, China. 

I graduated from the School of Automation at Southeast University (**[SEU, 985](https://www.seu.edu.cn/)**) with a Bachelor's degree in Robotics Engineering (**GPA Rank: 2/34**).

My research interests includeï¼šunderwater 3D vision, multi-sensor fusion SLAM, autonomous robot navigation.


---
# ğŸ”¥ News
---
<!-- 
- *2024.06.14*: &nbsp; Invited by TIV to review a paper related to autonomous drivingğŸ‰ğŸ‰
- *2024.05.30*: &nbsp; ğŸ‰ğŸ˜ŠRecognized as an outstanding student leader of UCAS
- *2024.03.21*: &nbsp; Publish a paper in TSMC!ğŸ˜ŠğŸ˜Š
- *2023.11.25*: &nbsp;ğŸ‰A paper accepted by TIV!ğŸ˜Š
- *2023.10.06*: &nbsp; A paper accepted by TII!ğŸ‰
- *2021.06.15*: &nbsp;ğŸ‰ğŸ‰Invited by IEEE Sensor to review a paper related to camera-imu calibrationğŸ˜Š
- *2023.03.27*: &nbsp;ğŸ˜Š Publish a paper in TIM!ğŸ˜Š
- *2023.03.09*: &nbsp;ğŸ‰ Successfully selected for the PHD experimental class of CASIA!ğŸ‰
- *2021.07.29*: &nbsp;ğŸ˜Š Participation in the China Control Conference in Shanghai,China!ğŸ‰ğŸ‰
- *2021.06.31*: &nbsp;ğŸ‰ğŸ‰ Successfully graduate from SEU and go to Beijing to start PHD!ğŸ˜Š-->
<div style="max-height: 150px; overflow-y: auto; font-family: 'Times New Roman', Times, serif; font-size: 11.5pt;">

2024.06.14: &nbsp; Invited by TIV to review a paper related to autonomous drivingğŸ‰ğŸ‰  
<br>
2024.05.30: &nbsp; ğŸ‰ğŸ˜ŠRecognized as an outstanding student leader of UCAS  
<br>
2024.03.21: &nbsp; Publish a paper in TSMC!ğŸ˜ŠğŸ˜Š  
<br>
2023.11.25: &nbsp;ğŸ‰A paper accepted by TIV!ğŸ˜Š  
<br>
2023.10.06: &nbsp; A paper accepted by TII!ğŸ‰  
<br>
2021.06.15: &nbsp;ğŸ‰ğŸ‰Invited by IEEE Sensor to review a paper related to camera-imu calibrationğŸ˜Š  
<br>
2023.03.27: &nbsp;ğŸ˜Š Publish a paper in TIM!ğŸ˜Š  
<br>
2023.03.09: &nbsp;ğŸ‰ Successfully selected for the PHD experimental class of CASIA!ğŸ‰  
<br>
2021.07.29: &nbsp;ğŸ˜Š Participation in the China Control Conference in Shanghai,China!ğŸ‰ğŸ‰  
<br>
2021.06.31: &nbsp;ğŸ‰ğŸ‰ Successfully graduate from SEU and go to Beijing to start PHD!ğŸ˜Š  

</div>


---
# ğŸ“– Educations
---
- *2021.09 - 2026.06(expected)*:&nbsp; Institute of Automation, Chinese Academy of Sciences (**Ph.D.**)
- *2021.09 - 2022.06*:&nbsp; School of Artificial Intelligence, University of Chinese Academy of Sciences (**Ph.D.**)
- *2018.06 - 2021.06*:&nbsp; Department of Robotics Engineering, School of Automation, Southeast University (**B.E.**)
- *2017.09 - 2018.06*:&nbsp; School of Materials Science and Engineering, Southeast University (**B.E.**)

---
# ğŸ“ Publications 
---
For a more detailed presentation, see [Publications](https://ouyaming.github.io/publications/) and [Projects](https://ouyaming.github.io/projects/).

(**<sup>*</sup>** indicates corresponding author)
1. **Y. Ou** et al., "Structured Light-Based Underwater Collision-Free Navigation and Dense Mapping System for Refined Exploration in Unknown Dark Environments," in IEEE Transactions on Systems, Man, and Cybernetics: Systems, doi: 10.1109/TSMC.2024.3370917. (**SCI, JCR Q1, ä¸­ç§‘é™¢ä¸€åŒºTop, IF=8.7**)
2. **Y. Ou**, J. Fan, C. Zhou, L. Cheng and M. Tan, "Water-MBSL: Underwater Movable Binocular Structured Light-Based High-Precision Dense Reconstruction Framework," in IEEE Transactions on Industrial Informatics, vol. 20, no. 4, pp. 6142-6154, April 2024, doi: 10.1109/TII.2023.3342899. (**SCI, JCR Q1, ä¸­ç§‘é™¢ä¸€åŒºTop, IF=12.3**)
3. **Y. Ou**, J. Fan, C. Zhou, S. Tian, L. Cheng and M. Tan, "Binocular Structured Light 3-D Reconstruction System for Low-Light Underwater Environments: Design, Modeling, and Laser-Based Calibration," in IEEE Transactions on Instrumentation and Measurement, vol. 72, pp. 1-14, 2023, Art no. 5010314, doi: 10.1109/TIM.2023.3261941. (**SCI, JCR Q1, ä¸­ç§‘é™¢äºŒåŒºTop, IF=5.6**)
4. J. Fan, **Y. Ou<sup>*</sup>**, X. Li, C. Zhou and Z. Hou, "Structured light vision based pipeline tracking and 3D reconstruction method for underwater vehicle," in IEEE Transactions on Intelligent Vehicles, doi: 10.1109/TIV.2023.3340737. (**corresponding author, SCI, JCR Q1, ä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=8.2**)
5. **Y. Ou**, Z. Zhang, C. Zhou and B. Zhou, "Data Calibration Algorithm for Artificial Lateral Line Sensor of Robotic Fish on Improved LSTM," 2021 40th Chinese Control Conference (CCC), Shanghai, China, 2021, pp. 4308-4314, doi: 10.23919/CCC52363.2021.9549820. (**EI**)
6. **Y. Ou**, J. Fan, et al. "Hybrid-VINS: Underwater Tightly-Coupled Hybrid Visual Inertial Dense SLAM for AUV," in IEEE Transactions on Industrial Electronics, 2024. (**Major Revision, SCI, JCR Q1, ä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=7.6**)

---
# ğŸ›¡ï¸ Patents 
---
1. Zhou C, **Ou Y**, Fan J, et al, Underwater Mobile Dense Mapping Platform and Mapping Method Based on Binocular Structured Light. Chinese Patent, CN117893675A.
2. Zhou C, **Ou Y**, Fan J, et al, A Simultaneous Localization System and Method for Underwater Robots. Chinese Patent, KHP2411116437.0.
3. Zhang Z, Zhou C, Fan J, **Ou Y**, Bionic Lateral Line Sensor. Chinese Patent, CN114624461B.
4. Zhang Z, Zhou C, Fan J, ..., **Ou Y**. Calibration Model Training Method, Device and System, Electronic Equipment and Storage Medium. Chinese Patent, CN117634641A.

---
# ğŸ† Honors & Awards
---
- *2024*:&nbsp; ğŸ“œOutstanding Student Leader Award of UCAS (**top 2%**)
- *2023*:&nbsp; ğŸ“œSelected for the PHD experimental class of CASIA (**8/276, 5W RMB scholarships each year**)
- *2022*:&nbsp; ğŸ¥‡China ICV Algorithms Challenge Competition(**1st place, 2W RMB award**)
- *2022*:&nbsp; ğŸ“œThree Good Students Award of UCAS (**top 10%**)
- *2021*:&nbsp; ğŸ†Data Application Innovation and Entrepreneurship Competition (**merit award, 9/453, 2.5K RMB prize**)
- *2021*:&nbsp; ğŸ“œOutstanding Graduate Student Award of Southeast University (**top 3%**)
- *2020*:&nbsp; ğŸ¥ˆNational College Students Intelligent Vehicle Competition (**national 2nd prize**)
- *2020*:&nbsp; ğŸ“œChina National Inspiration Scholarship (**3.22%**)
- *2019*:&nbsp; ğŸ¥‡RoboCup Robotics World Cup (China Region) (**national 1st prize**)
- *2019*:&nbsp; ğŸ¥ˆNational College Students Electronic Design Competition (**national 2nd prize**)
- *2019*:&nbsp; ğŸ¥‡The 10th Robot Competition of Jiangsu Province (**provincial 1st prize**)
- *2019*:&nbsp; ğŸ“œThree Good Students Award of Southeast University (**top 10%**)
- *2018*:&nbsp; ğŸ¥‡The 9th Robot Competition of Jiangsu Province (**provincial 1st prize**)
- *2016*:&nbsp; ğŸ¥ˆHigh School Mathematics Olympiad Competition, Anhui Province (**provincial 2nd prize**)

---
# ğŸ› ï¸ Projects & Competitions
---
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- TIE -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <p style="text-align:center">
          <video style="width:100%;height:auto;" controls="controls">
            <source src="https://ouyaming.github.io/vedio/2024-TIE-vedio.mp4" type="video/mp4" />
          </video>
        </p>
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="" target="_blank">
          <papertitle>Hybrid-VINS: Underwater Tightly-Coupled Hybrid Visual Inertial Dense SLAM for AUV</papertitle>
        </a >
        <br>
        <strong>Yaming Ou</strong>, Junfeng Fan, Chao Zhou, etc.
        <br>
        <span style="font-size: 10pt;">Submitted in <em>IEEE Transactions on Industrial Electronics</em>, 2024</span>
        <br>
        <!--  -->
        <p>An underwater tightly-coupled hybrid visual inertial dense SLAM framework, named Hybrid-VINS, is proposed, which is the first underwater SLAM system to utilize active vision information to assist passive vision.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">SLAM</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Camera-IMU Fusion</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/vedio/2024-TIE-vedio.mp4" target="_blank">[Video]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- TSMC -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <p style="text-align:center">
          <video style="width:100%;height:auto;" controls="controls">
            <source src="https://ouyaming.github.io/vedio/2024-03-18-TSMC_vedio.mp4" type="video/mp4" />
          </video>
        </p>
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2024-03-18-TSMC" target="_blank">
          <papertitle>Structured Light-Based Underwater Collision-Free Navigation and Dense Mapping System for Refined Exploration in Unknown Dark Environments</papertitle>
        </a >
        <br>
        <strong>Yaming Ou</strong>, Junfeng Fan, Chao Zhou, etc.
        <br>
        <span style="font-size: 10pt;">Published in <em>IEEE Transactions on Systems, Man, and Cybernetics: Systems</em>, 2024</span>
        <br>
        <!--  -->
        <p>A more adaptable 3D dense mapping robotic system based on self-designed scanning BSL, named ROVScanner, is developed for refined exploration, where the on-board design allows for autonomous mobility and operational capabilities.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Navigation</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Obstacle Avoidance</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/files/2024-03-18-TSMC.pdf" target="_blank">[Paper]</a>
          <a href="https://ouyaming.github.io/vedio/2024-03-18-TSMC_vedio.mp4" target="_blank">[Video]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- TII -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <p style="text-align:center">
          <video style="width:100%;height:auto;" controls="controls">
            <source src="https://ouyaming.github.io/vedio/2023-12-29-TII_vedio.mp4" type="video/mp4" />
          </video>
        </p>
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2023-12-29-TII" target="_blank">
          <papertitle>Water-MBSL: Underwater Movable Binocular Structured Light-Based High-Precision Dense Reconstruction Framework</papertitle>
        </a >
        <br>
        <strong>Yaming Ou</strong>, Junfeng Fan, Chao Zhou, etc.
        <br>
        <span style="font-size: 10pt;">Published in <em>IEEE Transactions on Industrial Informatics</em>, 2023</span>
        <br>
        <!--  -->
        <p>An SSLSLS-based underwater movable high-precision dense reconstruction framework, named Water-movable binocular structured light (MBSL), is proposed, which takes into account both motion distortion and point cloud registration.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Dense Mapping</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Motion Reconstruction</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/files/2023-12-29-TII.pdf" target="_blank">[Paper]</a>
          <a href="https://ouyaming.github.io/vedio/2023-12-29-TII_vedio.mp4" target="_blank">[Video]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- TIM -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <img src="https://ouyaming.github.io/images/500x300.png" alt="clean-usnob" width="500" height="300">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2023-03-27-TIM" target="_blank">
          <papertitle>Binocular Structured Light 3-D Reconstruction System for Low-light Underwater Environments: Design, Modeling, and Laser-based Calibration</papertitle>
        </a >
        <br>
        <strong>Yaming Ou</strong>, Junfeng Fan, Chao Zhou, etc.
        <br>
        <span style="font-size: 10pt;">Published in <em>IEEE Transactions on Instrumentation and Measurement</em>, 2023</span>
        <br>
        <!--  -->
        <p>An underwater binocular structured light 3-D reconstruction system with the scanning laser is designed to realize the static high-precision scanning reconstruction of the low-light scene, which is suitable for underwater robot application.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">3D Reconstruction</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Active Vision</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/files/2023-03-27-TIM.pdf" target="_blank">[Paper]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- TIV -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <img src="https://ouyaming.github.io/images/500x300.png" alt="clean-usnob" width="500" height="300">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2023-03-27-TIM" target="_blank">
          <papertitle>Structured Light Vision-Based Pipeline Tracking and 3D Reconstruction Method for Underwater Vehicle</papertitle>
        </a >
        <br>
        Junfeng Fan, <strong>Yaming Ou</strong>,  Xuan Li, etc.
        <br>
        <span style="font-size: 10pt;">Published in <em>IEEE Transactions on Intelligent Vehicles</em>, 2023</span>
        <br>
        <!--  -->
        <p>A novel underwater pipeline positioning method based on dual-line laser SLV is proposed, which can simultaneously obtain the lateral deviation, height deviation and heading deviation of underwater vehicle and underwater pipeline under weak light water environment, providing the basis for underwater pipeline tracking.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">3D Reconstruction</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Pipeline Tracking</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/files/2023-12-08-TIV.pdf" target="_blank">[Paper]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- ç™¾åº¦AIæ™ºèƒ½è½¦ -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <p style="text-align:center">
          <video style="width:100%;height:auto;" controls="controls">
            <source src="https://ouyaming.github.io/vedio/project_AI_smart_car_vedio.mp4" type="video/mp4" />
          </video>
        </p>
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2023-12-29-TII" target="_blank">
          <papertitle>Self-Driving Vehicle</papertitle>
        </a >
        <br>
        <strong>Competition</strong> <em>National College Students Intelligent Vehicle Competition</em>
        <br>
        <strong>Award</strong> <em>National Second Prize</em>
        &nbsp;&nbsp;
        <strong>Sponsor</strong>
        <em><a href="https://www.baidu.com/" target="_blank">Baidu</a></em>
        <br>
        <!--  -->
        <p>Vehicle Autonomous Driving according to different traffic signs.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Autonomous Driving</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Target Detection</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/vedio/project_AI_smart_car_vedio.mp4" target="_blank">[Video]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;"> <!-- LKA -->
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <p style="text-align:center">
          <video style="width:100%;height:auto;" controls="controls">
            <source src="https://ouyaming.github.io/vedio/project_LKA_vedio.mp4" type="video/mp4" />
          </video>
        </p>
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2023-12-29-TII" target="_blank">
          <papertitle>Lane Keeping Assist (LKA) Control Based on the LQR Controller</papertitle>
        </a >
        <br>
        <strong>Competition</strong> <em>China ICV Algorithms Challenge</em>
        <br>
        <strong>Award</strong> <em>First Prize</em>
        &nbsp;&nbsp;
        <strong>Sponsor</strong>
        <em><a href="https://www.china-icv.cn/" target="_blank">China-ICV</a></em>
        <br>
        <p> Lateral lane keeping and speed keeping in different scenarios.</p >
        <p>
          <strong>Tags</strong>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">Autonomous Driving</span>
          <span style="background-color: #f0f0f0; padding: 3px 8px; border-radius: 5px; margin-right: 5px;">LQR Control</span>
          &nbsp;&nbsp;
          <strong>Source</strong>
          <a href="https://ouyaming.github.io/vedio/project_LKA_vedio.mp4" target="_blank">[Video]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>









<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size: 11.5pt;">
  <tbody>
    <tr>
      <td style="padding:20px;width:40%;vertical-align:middle;border:none;">
        <img src="https://ouyaming.github.io/images/500x300.png" alt="clean-usnob" width="500" height="300">
      </td>
      <td style="padding:20px;width:60%;vertical-align:middle;border:none;">
        <a href="https://ouyaming.github.io/publication/2024-03-18-TSMC" target="_blank">
          <papertitle>Hybrid-VINS: Underwater Tightly-Coupled Hybrid Visual Inertial Dense SLAM for AUV</papertitle>
        </a >
        <br>
        <strong>Y. Ou</strong>,
        J. Fan
        <br>
        Submitted in <span style="white-space: nowrap; font-size: 1em;"><em>IEEE Transactions on Industrial Electronics</em>, 2024</span>
        <br>
        <!--  -->
        <p>The proposed model represents a significant advancement in the field of metameric robotics and has the potential to enhance the performance of earthworm-like robots in a variety of challenging environments.</p >
      </td>
    </tr>
  </tbody>
</table>
